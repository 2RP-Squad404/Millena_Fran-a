# Relatório de Estudos
*Nome do Estágiario:* Millena França
*Data:* 20/08

*Módulos/Etapas Feitas:*  
1. **[Computação em Núvem](#computação-em-núvem)**
   - **[Google Cloud Dataflow](#google-cloud-dataflow)**
   - **[Google Cloud Dataproc](#google-cloud-dataproc)**
   - **[Google Cloud Composer](#google-cloud-composer)**
   - **[Google Cloud Functions](#google-cloud-functions)**
2. **[Linux/Shell](#linux/shell)**
     - **[Shell Script](#shell-script)**
## Resumo dos módulos 
## Computação em Núvem

### Google Cloud Dataflow
Serviço gerenciado para processamento e análise de dados em larga escala. Ele é projetado para executar pipelines de processamento de dados em tempo real e em batch de forma eficiente e escalável

#### Características
- **Modelo de Programação:** Baseado no Apache Beam, que permite criar pipelines de processamento de dados usando um modelo de programação unificado para processamento em batch e streaming.
- **Gerenciamento Automático de Recursos:** gerencia automaticamente os recursos necessários para a execução de pipelines.
- **Flexibilidade de Processamento:** Suporta processamento de dados em tempo real e em batch, permitindo que você defina e execute pipelines para diferentes tipos de processamento de dados.
- **Integração com o Ecossistema Google Cloud:** Integra-se facilmente com outros serviços do Google Cloud, como BigQuery, Cloud Storage e Pub/Sub, facilitando a ingestão, processamento e análise de dados.
- **Visualização e Monitoramento:** Oferece uma interface para monitorar a execução dos pipelines, visualizar métricas e logs, e diagnosticar problemas.

#### Componentes
- **Pipeline:** série de transformações que processam os dados de entrada e produzem dados de saída
- **Transformações:** filtragem, agregação, junção.
- **PCollections:** Coleções de dados que são manipuladas pelas transformações dentro de um pipeline. Representam dados intermediários e resultados finais.
- **Data Sources e Sinks:** Fontes e destinos dos dados. Fontes (data sources) podem ser serviços de armazenamento, bancos de dados ou fluxos de dados em tempo real, enquanto destinos (sinks) são os locais onde os dados processados são armazenados ou utilizados.
- **Workers:** Instâncias que executam as tarefas de processamento dos pipelines. O Dataflow gerencia automaticamente a criação e o escalonamento dos workers conforme necessário.

#### Aplicações
- Processar e analisar dados em tempo real
- Extrair dados e transformá-los quando necessário
- Automatizar fluxo de dados entre sistemas e serviços

### Google Cloud Dataproc
É um serviço gerenciado para processamento de dados em larga escala utilizando Apache Hadoop e Apache Spark, visando facilitaar o gerenciamento, execução e escalabilidade do processamento.

#### Características
- **Gerenciamento de Clusters:** O Dataproc gerencia automaticamente o ciclo de vida dos clusters, incluindo a criação, configuração, escalabilidade e a terminação, reduzindo a complexidade e o esforço manual.
- **Integração com o Google Cloud:** Integra-se facilmente com outros serviços do Google Cloud, como BigQuery, Cloud Storage, e Cloud Pub/Sub, facilitando a ingestão, o processamento e a análise de dados.
- **Escalabilidade:** Permite a escalabilidade automática e manual dos clusters para atender a diferentes cargas de trabalho, otimizando a utilização de recursos e o custo.
- **Eficiência de Custo:** O Dataproc permite a criação de clusters sob demanda e a execução de trabalhos em clusters efêmeros, ajudando a controlar os custos ao pagar apenas pelos recursos realmente utilizados.
- **Suporte a Vários Frameworks:** Suporta frameworks de processamento de dados populares, como Apache Hadoop, Apache Spark, Apache Hive e Apache HBase, oferecendo flexibilidade para diferentes necessidades de processamento.
- **Monitoramento e Logs:** Oferece ferramentas integradas para monitorar o desempenho dos clusters e acessar logs de execução, facilitando a identificação e a resolução de problemas.

- #### Componentes
  - **Cluster:** conjunto de máquinas virtuais que executam tarefas de processamento de dados
  - **Jobs:** Tarefas de processamento de dados enviadas para o cluster
  - **Nodes:** Máquinas virtuais dentro do cluster que executam tarefas de processamento


  #### Aplicações
  - **Processamento de Dados:** tarefas que exigem processamento de grande volume de dados
  - **Análise de Dados:** executar análises complexas Apache Spark e Apache Hive para extrair insights de grandes conjuntos de dados
  - **Machine Learning:** treinar modelos e executar algoritmos de aprendizado de máquina em dados grandes
 
  ### Google Cloud Composer
  É um serviço gerenciado de orquestração de workflows baseado em Apache Airflow. É é ideal para coordenar tarefas e processos de dados em ambientes de big data, automação de pipelines ETL, e integrações de sistemas.

  #### Características
  - **Baseado em Apache Airflow:** O Cloud Composer utiliza Apache Airflow, uma plataforma popular de orquestração de workflows, proporcionando flexibilidade e poderosas capacidades de automação para gerenciar pipelines de dados.
  - **Gerenciamento Automatizado:** Oferece gerenciamento automatizado dos recursos necessários para executar workflows, incluindo provisionamento, escalabilidade e manutenção do ambiente de execução.
  - **Integração com Google Cloud:** Facilita a integração com outros serviços do Google Cloud, como BigQuery, Cloud Storage, Dataflow e Dataproc, permitindo a criação de pipelines de dados coesos e eficientes.
  - **Escalabilidade:** Gerencia automaticamente a escalabilidade dos recursos necessários para a execução dos workflows, ajustando-se conforme a demanda.
  - **Interface de Usuário:** Fornece uma interface gráfica baseada em web para visualização, monitoramento e gerenciamento de workflows, facilitando a interação com os pipelines.
  - **Segurança e Compliance:** Oferece suporte a práticas de segurança e compliance, integrando-se com ferramentas e políticas do Google Cloud para proteger dados e operações.
 
  #### Componentes
  - **DAG (Directed Acyclic Graph):** Representação de um grafo direcionado acíclico que define a sequência e as dependências das tarefas dentro de um pipeline.
  - **Tarefas:** Operação ou um passo específico dentro do pipeline.
  - **Executores:** Componentes que executam as tarefas definidas nos DAGs. O Cloud Composer gerencia os executores automaticamente, garantindo que as tarefas sejam realizadas conforme o planejado.
  - **Operadores:** Blocos de construção para tarefas em Airflow. Eles encapsulam a lógica para interagir com diferentes sistemas e serviços, como executar scripts, enviar e-mails ou interagir com APIs.
  - **Sensors:** Tipos de operadores que aguardam a ocorrência de um evento ou a disponibilidade de um recurso antes de prosseguir com a execução das tarefas seguintes.
  - **Hooks:** Interfaces para se conectar e interagir com sistemas externos, como bancos de dados e APIs, permitindo a integração com diversos serviços e sistemas.
 
  ### Google Cloud Functions
  Serviço para executar código em resposta a eventos sem precisar gerenciar ou provisionar servidores

  #### Características
  - **Serverless:** Execução e dimensionamento automáticos da infraestrutura (servidores, clusters)
  -  **Escalabilidade:** Escala de acordo com a demanada
  -  **Execução por Eventos:** Executa funções em resposta a eventos específicos (ex: mudanças no Cloud Storage, solicitações HTTP etc)
  -  **Tempo de Execução:** Execução rápida e leve
 
  #### Componentes
  - **Funçoes:** Códigos que são executados em resposta a eventos específicos
  - **Triggers:** Eventos que acionam funções
  - **Eventos:** Dados que acompanham os triggers e ativam as funções
  - **Ambiente de Execução:** Local onde a função é executada
 
  #### Aplicações
  - **Automação de Processos:** Automatizar tarefas
  - **Reação a Eventos:** Responde a eventos em tempo real
  - **Processamento de Arquivos:** Automatizar o processo de arquivos armazenados no Cloud Storage

  Solução para execução de código para resposta a eventos

## Linux/Shell
### Shell Script
Utilizado para automatizar tarefas repetitivas e simplificar execução de comandos complexos

#### Conceitos
1. **Shell**: Programa que fornece uma interface de linha de comando para interagir com o sistema operacional. Exemplos de shells incluem Bash (Bourne Again Shell), Zsh (Z Shell) e Sh (Bourne Shell).
2. **Script**: Arquivo que contém uma sequência de comandos que são executados pelo shell. Ele pode incluir variáveis, loops, condicionais e funções para executar tarefas automatizadas.
3. **Comandos**: Shell scripts consistem em comandos que o shell pode executar, como `ls` para listar arquivos, `grep` para buscar padrões, e `cp` para copiar arquivos.
4. **Variáveis**: Variáveis em shell scripts são usadas para armazenar dados que podem ser utilizados em comandos e operações. Elas são definidas com um nome e podem armazenar valores como strings ou números.
5. **Loops e Condicionais**: Shell scripts podem incluir estruturas de controle, como loops (`for`, `while`) e condicionais (`if`, `else`), para executar comandos repetidamente ou tomar decisões com base em condições.
6. **Funções**: Funções permitem encapsular um conjunto de comandos em um bloco reutilizável, facilitando a modularização e a manutenção do script.

#### Exemplo de Shell Script

**Script de Monitoramento de Processos**:
    
    ```bash
    bashCopiar código
    #!/bin/bash
    if pgrep "processo_exemplo" > /dev/null
    then
        echo "O processo está em execução."
    else
        echo "O processo não está em execução."
    fi
    
    ```







*Desafios Encontrados:*  
Compreender alguns temas específicos.

*Feedback e Ajustes:*  
Mais indicações de materiais para o estudo dos assuntos, principalmente artigos.

*Próximos Passos:*  
Continuar estudano os temas e aguardar as demandas que serão passadas
