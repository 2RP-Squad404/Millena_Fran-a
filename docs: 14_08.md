# Relatório de Estudos
**Nome do Estágiario:** Millena França
**Data:** 09/08

**Módulos/Etapas Feitas:**  
1. **[Linguagens e Frameworks](#linguagens-e-frameworks)**
2. **[Python Framkework](#python-framework)**

## Resumo dos módulos 
## Linguagens e Frameworks
### Python - Introdução
Python é uma linguagem de prorgamação de alto nível, simples e intuitivo, facilitando sua leitura e escrita de código.

### Características
- **Sintaxe Simples:** acessivel para iniciantes por sua facil interpretação, ja que é facil de ler e escrever.
- **Portabilidade:** pode ser executada em várias plataformas sem necessidade de recompilação
- **Bibliotecas:** possui uma vasta variedad de bibliotecas e frameworks, facilitando o desenvolvimento de várias aplicações

### Usos Comuns
Alguns dos usos mais comuns do Python incluem desenvolvimento web, automação, testagem de software, análise de dados, machine learning e desenvolvimento de jogos.

### Python Framework
É uma estrutura de desenvolvimento de softeare que fornece uma base para contruir aplicativos e sistemas. É uma coleção de bibliotecas, módulos e ferramentas que ajudam os desenvolvedores
a criar aplicações de forma mais eficiente e rápida, seguindo as melhores práticas de programação. 

### Apache Spark
É um framework de processamento de dados em grande escala, proporciona um desempenho significativamente mais rápido em comparação com outras tecnologias como Hadoop MapReduce
PySpark é a interface do Apache Spark para a linguagem de programação Python. Ele permite que os desenvolvedores utilizem a poderosa engine de processamento de dados do Spark usando a
sintaxe e as bibliotecas do Python.

#### Características
- **Processamento em Memória:** com o processamento em memória fornece em um desepenho muito mais rápido para aplicações, worloads iterativas e interativas
- **Distribuição:** pode escalar de um único servidor para milhares de nós de computação, processando petabytes de dados
- **Suporte de Linguagens:** Spark suporta APIs em python, java, Scala, e R, oferecendo flexibilidade aos desenvolvedores
- **Integração:** boa integração com outras ferramentas do ecossistema BigData
- **API Unificada para Diferentes Tipos de Análise:** Spark oferece APIs para processamento de dados estruturados (Spark SQL), streaming (Spark Streaming),
machine learning (MLlib), e processamento de grafos (GraphX).

#### Usos Comuns
- Análise de Dados em Tempo Real
- Machine Learning
- ETC (Extract, Transform, Load)
- Análise de Big Data
- Processamento de Grafos

#### Benefícios
- Desempenho
- Versatilidade
- Comunidade Ativa

### Apache Beam
É um modelo de programação unificado e de código aberto para processamento de dados em lote e em tempo real. Permite a definição e execução de pipelines de dados usando uma API consistente.

#### Características
- **Portabilidade:** Permite que você escreva pipelines uma vez e os execute em diferentes sistemas
- **Modelo Unificado:** para processamento de dados em lote e streaming
- **Transformações:** inclui um conjunto rico de transformações para manipular dados
- **Janelas e Watermarks:** suporte a processamentos em tempo real, conceitos de janelas e marcações de água para lidar com dados fora de ordem

### Google DataFlow
Serviço  da Google Cloud que executa pipelines criados com Apache Beam

#### Características
- **Gerenciamento:** o Google Dataflow cuida  da escalabilidade, balanceamento de carga e manutenção
- **Integração:** se integra com os outros seriviços do Google Cloud
- **Custo:** preço baseado em tempo de processamento de volume de dados

### Apache Airflow
É uma plataforma de código aberto para orquestração e automação de workflows de dados
- Workflow é o método usado para organizar o fluxo de trabalho em uma empresa, consistindo na disposição e realização das atividades em sequÊncia lógica, preferencialmente de forma automatizada.

#### Componentes Principais
- **DAG (Directed Acyclic Graph):** Estrutura que define o fluxo de tarefas. Representação de como as tarefas se relacionam e a ordem em que devem ser executadas.
- **Operadores:** Componentes que definem as ações a serem executadas em cada tarefa, como BashOperator, PythonOperator, e muitos outros operadores específicos de serviços e sistemas.
- **Tasks:** Unidades de trabalho definidas dentro de um DAG. Cada tarefa representa uma ação específica a ser executada.
- **Scheduler:** Componente responsável por acionar as tarefas baseadas no cronograma definido e garantir que as tarefas sejam executadas conforme o esperado.
- **Executor:** Componente que lida com a execução real das tarefas. Pode ser configurado para usar diferentes backends, como Celery, Kubernetes, ou LocalExecutor.
- **Web Interface:** Interface gráfica para visualizar e gerenciar DAGs, monitorar o status das tarefas e visualizar logs de execução.

#### Exemplo de Usos
- **ETL (Extract, Transform, Load):** Orquestração de pipelines de ETL, gerenciando a extração de dados, transformação e carregamento em sistemas de armazenamento ou data warehouses.
- **Automação de Processos:** Automatização de tarefas recorrentes, como geração de relatórios, sincronização de dados, e execução de análises.


**Desafios Encontrados:**  
Compreender alguns temas específicos.

**Feedback e Ajustes:**  
Mais indicações de materiais para o estudo dos assuntos, principalmente artigos.

**Próximos Passos:**  
Vou me aprofundar na parte de Linguagens e Frameworks
