# Relatório de Estudos
**Nome do Estágiario:** Millena França
**Data:** 09/08

**Módulos/Etapas Feitas:**  
1. **[Linguagens e Frameworks](#linguagens-e-frameworks)**
   - **[Python Framkework](#python-framework)**
   - **[Apache Spark](#apache-spark)**
   - **[Apache Beam](#apache-beam)**
   - **[Google Dataflow](#google-dataflow)**
   - **[Apache Airflow](#apache-airflow)**
2. **[Virtualização](#virtualizacao)**

## Resumo dos módulos 
## Linguagens e Frameworks
### Python - Introdução
Python é uma linguagem de prorgamação de alto nível, simples e intuitivo, facilitando sua leitura e escrita de código.

### Características
- **Sintaxe Simples:** acessivel para iniciantes por sua facil interpretação, ja que é facil de ler e escrever.
- **Portabilidade:** pode ser executada em várias plataformas sem necessidade de recompilação
- **Bibliotecas:** possui uma vasta variedad de bibliotecas e frameworks, facilitando o desenvolvimento de várias aplicações

### Usos Comuns
Alguns dos usos mais comuns do Python incluem desenvolvimento web, automação, testagem de software, análise de dados, machine learning e desenvolvimento de jogos.

### Python Framework
É uma estrutura de desenvolvimento de softeare que fornece uma base para contruir aplicativos e sistemas. É uma coleção de bibliotecas, módulos e ferramentas que ajudam os desenvolvedores
a criar aplicações de forma mais eficiente e rápida, seguindo as melhores práticas de programação. 

### Apache Spark
É um framework de processamento de dados em grande escala, proporciona um desempenho significativamente mais rápido em comparação com outras tecnologias como Hadoop MapReduce
PySpark é a interface do Apache Spark para a linguagem de programação Python. Ele permite que os desenvolvedores utilizem a poderosa engine de processamento de dados do Spark usando a
sintaxe e as bibliotecas do Python.

#### Características
- **Processamento em Memória:** com o processamento em memória fornece em um desepenho muito mais rápido para aplicações, worloads iterativas e interativas
- **Distribuição:** pode escalar de um único servidor para milhares de nós de computação, processando petabytes de dados
- **Suporte de Linguagens:** Spark suporta APIs em python, java, Scala, e R, oferecendo flexibilidade aos desenvolvedores
- **Integração:** boa integração com outras ferramentas do ecossistema BigData
- **API Unificada para Diferentes Tipos de Análise:** Spark oferece APIs para processamento de dados estruturados (Spark SQL), streaming (Spark Streaming),
machine learning (MLlib), e processamento de grafos (GraphX).

#### Usos Comuns
- Análise de Dados em Tempo Real
- Machine Learning
- ETC (Extract, Transform, Load)
- Análise de Big Data
- Processamento de Grafos

#### Benefícios
- Desempenho
- Versatilidade
- Comunidade Ativa

### Apache Beam
É um modelo de programação unificado e de código aberto para processamento de dados em lote e em tempo real. Permite a definição e execução de pipelines de dados usando uma API consistente.

#### Características
- **Portabilidade:** Permite que você escreva pipelines uma vez e os execute em diferentes sistemas
- **Modelo Unificado:** para processamento de dados em lote e streaming
- **Transformações:** inclui um conjunto rico de transformações para manipular dados
- **Janelas e Watermarks:** suporte a processamentos em tempo real, conceitos de janelas e marcações de água para lidar com dados fora de ordem

### Google DataFlow
Serviço  da Google Cloud que executa pipelines criados com Apache Beam

#### Características
- **Gerenciamento:** o Google Dataflow cuida  da escalabilidade, balanceamento de carga e manutenção
- **Integração:** se integra com os outros seriviços do Google Cloud
- **Custo:** preço baseado em tempo de processamento de volume de dados

### Apache Airflow
É uma plataforma de código aberto para orquestração e automação de workflows de dados
- Workflow é o método usado para organizar o fluxo de trabalho em uma empresa, consistindo na disposição e realização das atividades em sequÊncia lógica, preferencialmente de forma automatizada.

#### Componentes Principais
- **DAG (Directed Acyclic Graph):** Estrutura que define o fluxo de tarefas. Representação de como as tarefas se relacionam e a ordem em que devem ser executadas.
- **Operadores:** Componentes que definem as ações a serem executadas em cada tarefa, como BashOperator, PythonOperator, e muitos outros operadores específicos de serviços e sistemas.
- **Tasks:** Unidades de trabalho definidas dentro de um DAG. Cada tarefa representa uma ação específica a ser executada.
- **Scheduler:** Componente responsável por acionar as tarefas baseadas no cronograma definido e garantir que as tarefas sejam executadas conforme o esperado.
- **Executor:** Componente que lida com a execução real das tarefas. Pode ser configurado para usar diferentes backends, como Celery, Kubernetes, ou LocalExecutor.
- **Web Interface:** Interface gráfica para visualizar e gerenciar DAGs, monitorar o status das tarefas e visualizar logs de execução.

#### Exemplo de Usos
- **ETL (Extract, Transform, Load):** Orquestração de pipelines de ETL, gerenciando a extração de dados, transformação e carregamento em sistemas de armazenamento ou data warehouses.
- **Automação de Processos:** Automatização de tarefas recorrentes, como geração de relatórios, sincronização de dados, e execução de análises.

é uma ferramenta poderosa para a automação e orquestração de workflows de dados, oferecendo flexibilidade, escalabilidade e uma interface visual rica para gerenciamento e monitoramento. É amplamente utilizado em ambientes de dados complexos e para a execução e monitoramento de pipelines de dados e processos automatizados.

### Resumo das Funcionalidades
- **Apache Spark:** Focado em processamento de dados em larga escala, tanto em batch quanto em streaming.
- **Apache Beam:** Framework para criar pipelines de dados que podem ser executados em diferentes engines.
- **Google Dataflow:** Serviço gerenciado que executa pipelines Apache Beam na Google Cloud.
- **Apache Airflow:** Orquestrador de workflows para gerenciar a execução de tarefas e pipelines de dados.


### Virtualização
Se trata de Máquinas Virtuais (VMs) que são emulações de computadores físicos que rodam dentro de um ambiente de software.

#### Características
- **Recursos Compartilhados:** compartilham recursos físicos do hardware como CPU, memória e armazenamento, porém operam de forma independente
- **Portabilidade:** as VMs podem ser movidas entre diferentes servidores físicos permitindo flexibilidade e recuperação
- **Isolamento:** individualidade para cada VM, prmitindo execução de diferentes sistemas operacionais e aplicativos sem interferir entre si

#### Componentes
- **Hypervisor:** O software que cria e gerencia máquinas virtuais. Existem dois tipos principais de hypervisors:
  - Tipo 1 (Bare-metal): Executa diretamente no hardware do servidor, como VMware ESXi e Microsoft Hyper-V.
  - Tipo 2 (Hosted): Executa sobre um sistema operacional host, como VMware Workstation e Oracle VirtualBox.
- **Imagens de Máquina Virtual:** Arquivos que contêm o sistema operacional, aplicativos e dados necessários para iniciar e operar uma máquina virtual.
- **Virtualização de Hardware:** A camada de virtualização emula os recursos de hardware (como CPUs, memória, e discos) para as máquinas virtuais, permitindo que elas funcionem como se estivessem em um hardware físico dedicado.
- **Recursos Virtuais:** Recursos como CPUs virtuais, memória virtual e armazenamento virtual são alocados para as máquinas virtuais conforme a necessidade.

Devido a sua acessibilidade é utilizado para uma gama de aplicações, aumentando a eficiência e reduzindo os custos com hardware, servindo para testes de aplicativos, execução de sistemas operacionais entre outras utilidades. 

#### Desafios
- **Overhead de Desempenho:** A virtualização pode introduzir algum overhead de desempenho devido à sobrecarga da camada de virtualização.
- **Gerenciamento de Recursos:** Requer planejamento e gerenciamento cuidadosos para garantir que os recursos sejam alocados e utilizados de maneira eficiente.
- **Complexidade:** A gestão de múltiplas máquinas virtuais pode adicionar complexidade ao ambiente de TI, exigindo ferramentas e práticas adequadas para administração.


**Desafios Encontrados:**  
Compreender alguns temas específicos.

**Feedback e Ajustes:**  
Mais indicações de materiais para o estudo dos assuntos, principalmente artigos.

**Próximos Passos:**  
Vou para a parte de computação em nuvem
