# Relatório de Estudos
**Nome do Estágiario:** Millena França
**Data:** 09/08

**Módulos/Etapas Feitas:**  
1. **[Linguagens e Frameworks](#linguagens-e-frameworks)**
   - **[Python Framkework](#python-framework)**
   - **[Apache Spark](#apache-spark)**
   - **[Apache Beam](#apache-beam)**
   - **[Google Dataflow](#google-dataflow)**
   - **[Apache Airflow](#apache-airflow)**
2. **[Virtualização](#virtualização)**
   - **[Docker](#docker)**
   - **[Kubernetes](#kubernetes)**
3. **[CI/CD](#cicd)**

## Resumo dos módulos 
## Linguagens e Frameworks
### Python - Introdução
Python é uma linguagem de prorgamação de alto nível, simples e intuitivo, facilitando sua leitura e escrita de código.

### Características
- **Sintaxe Simples:** acessivel para iniciantes por sua facil interpretação, ja que é facil de ler e escrever.
- **Portabilidade:** pode ser executada em várias plataformas sem necessidade de recompilação
- **Bibliotecas:** possui uma vasta variedad de bibliotecas e frameworks, facilitando o desenvolvimento de várias aplicações

### Usos Comuns
Alguns dos usos mais comuns do Python incluem desenvolvimento web, automação, testagem de software, análise de dados, machine learning e desenvolvimento de jogos.

### Python Framework
É uma estrutura de desenvolvimento de softeare que fornece uma base para contruir aplicativos e sistemas. É uma coleção de bibliotecas, módulos e ferramentas que ajudam os desenvolvedores
a criar aplicações de forma mais eficiente e rápida, seguindo as melhores práticas de programação. 

### Apache Spark
É um framework de processamento de dados em grande escala, proporciona um desempenho significativamente mais rápido em comparação com outras tecnologias como Hadoop MapReduce
PySpark é a interface do Apache Spark para a linguagem de programação Python. Ele permite que os desenvolvedores utilizem a poderosa engine de processamento de dados do Spark usando a
sintaxe e as bibliotecas do Python.

#### Características
- **Processamento em Memória:** com o processamento em memória fornece em um desepenho muito mais rápido para aplicações, worloads iterativas e interativas
- **Distribuição:** pode escalar de um único servidor para milhares de nós de computação, processando petabytes de dados
- **Suporte de Linguagens:** Spark suporta APIs em python, java, Scala, e R, oferecendo flexibilidade aos desenvolvedores
- **Integração:** boa integração com outras ferramentas do ecossistema BigData
- **API Unificada para Diferentes Tipos de Análise:** Spark oferece APIs para processamento de dados estruturados (Spark SQL), streaming (Spark Streaming),
machine learning (MLlib), e processamento de grafos (GraphX).

#### Usos Comuns
- Análise de Dados em Tempo Real
- Machine Learning
- ETC (Extract, Transform, Load)
- Análise de Big Data
- Processamento de Grafos

#### Benefícios
- Desempenho
- Versatilidade
- Comunidade Ativa

### Apache Beam
É um modelo de programação unificado e de código aberto para processamento de dados em lote e em tempo real. Permite a definição e execução de pipelines de dados usando uma API consistente.

#### Características
- **Portabilidade:** Permite que você escreva pipelines uma vez e os execute em diferentes sistemas
- **Modelo Unificado:** para processamento de dados em lote e streaming
- **Transformações:** inclui um conjunto rico de transformações para manipular dados
- **Janelas e Watermarks:** suporte a processamentos em tempo real, conceitos de janelas e marcações de água para lidar com dados fora de ordem

### Google DataFlow
Serviço  da Google Cloud que executa pipelines criados com Apache Beam

#### Características
- **Gerenciamento:** o Google Dataflow cuida  da escalabilidade, balanceamento de carga e manutenção
- **Integração:** se integra com os outros seriviços do Google Cloud
- **Custo:** preço baseado em tempo de processamento de volume de dados

### Apache Airflow
É uma plataforma de código aberto para orquestração e automação de workflows de dados
- Workflow é o método usado para organizar o fluxo de trabalho em uma empresa, consistindo na disposição e realização das atividades em sequÊncia lógica, preferencialmente de forma automatizada.

#### Componentes Principais
- **DAG (Directed Acyclic Graph):** Estrutura que define o fluxo de tarefas. Representação de como as tarefas se relacionam e a ordem em que devem ser executadas.
- **Operadores:** Componentes que definem as ações a serem executadas em cada tarefa, como BashOperator, PythonOperator, e muitos outros operadores específicos de serviços e sistemas.
- **Tasks:** Unidades de trabalho definidas dentro de um DAG. Cada tarefa representa uma ação específica a ser executada.
- **Scheduler:** Componente responsável por acionar as tarefas baseadas no cronograma definido e garantir que as tarefas sejam executadas conforme o esperado.
- **Executor:** Componente que lida com a execução real das tarefas. Pode ser configurado para usar diferentes backends, como Celery, Kubernetes, ou LocalExecutor.
- **Web Interface:** Interface gráfica para visualizar e gerenciar DAGs, monitorar o status das tarefas e visualizar logs de execução.

#### Exemplo de Usos
- **ETL (Extract, Transform, Load):** Orquestração de pipelines de ETL, gerenciando a extração de dados, transformação e carregamento em sistemas de armazenamento ou data warehouses.
- **Automação de Processos:** Automatização de tarefas recorrentes, como geração de relatórios, sincronização de dados, e execução de análises.

é uma ferramenta poderosa para a automação e orquestração de workflows de dados, oferecendo flexibilidade, escalabilidade e uma interface visual rica para gerenciamento e monitoramento. É amplamente utilizado em ambientes de dados complexos e para a execução e monitoramento de pipelines de dados e processos automatizados.

### Resumo das Funcionalidades
- **Apache Spark:** Focado em processamento de dados em larga escala, tanto em batch quanto em streaming.
- **Apache Beam:** Framework para criar pipelines de dados que podem ser executados em diferentes engines.
- **Google Dataflow:** Serviço gerenciado que executa pipelines Apache Beam na Google Cloud.
- **Apache Airflow:** Orquestrador de workflows para gerenciar a execução de tarefas e pipelines de dados.


### Virtualização
Se trata de Máquinas Virtuais (VMs) que são emulações de computadores físicos que rodam dentro de um ambiente de software.

#### Características
- **Recursos Compartilhados:** compartilham recursos físicos do hardware como CPU, memória e armazenamento, porém operam de forma independente
- **Portabilidade:** as VMs podem ser movidas entre diferentes servidores físicos permitindo flexibilidade e recuperação
- **Isolamento:** individualidade para cada VM, prmitindo execução de diferentes sistemas operacionais e aplicativos sem interferir entre si

#### Componentes
- **Hypervisor:** O software que cria e gerencia máquinas virtuais. Existem dois tipos principais de hypervisors:
  - Tipo 1 (Bare-metal): Executa diretamente no hardware do servidor, como VMware ESXi e Microsoft Hyper-V.
  - Tipo 2 (Hosted): Executa sobre um sistema operacional host, como VMware Workstation e Oracle VirtualBox.
- **Imagens de Máquina Virtual:** Arquivos que contêm o sistema operacional, aplicativos e dados necessários para iniciar e operar uma máquina virtual.
- **Virtualização de Hardware:** A camada de virtualização emula os recursos de hardware (como CPUs, memória, e discos) para as máquinas virtuais, permitindo que elas funcionem como se estivessem em um hardware físico dedicado.
- **Recursos Virtuais:** Recursos como CPUs virtuais, memória virtual e armazenamento virtual são alocados para as máquinas virtuais conforme a necessidade.

Devido a sua acessibilidade é utilizado para uma gama de aplicações, aumentando a eficiência e reduzindo os custos com hardware, servindo para testes de aplicativos, execução de sistemas operacionais entre outras utilidades. 

#### Desafios
- **Overhead de Desempenho:** A virtualização pode introduzir algum overhead de desempenho devido à sobrecarga da camada de virtualização.
- **Gerenciamento de Recursos:** Requer planejamento e gerenciamento cuidadosos para garantir que os recursos sejam alocados e utilizados de maneira eficiente.
- **Complexidade:** A gestão de múltiplas máquinas virtuais pode adicionar complexidade ao ambiente de TI, exigindo ferramentas e práticas adequadas para administração.

### Docker
É uma plataforma de software para a execução de aplicativos dentro de containers (ambientes isolados que contêm tudo o que um aplicativo precisa para funcionar ex: codigo, bibliotecas e depêndencias).

#### Características Principais
- **Containers:** encapsulam o aplicativo e suas depêndencias em um ambiente isolado. Compartilham o mesmo kernel do sistema operacional porém trabalham independentes, reduzindo overhead.
- **Imagens Docker:** modelos para criar containers, contém o código-fonte, bibliotecas e depêndencias para o aplicativo.
- **Dockerfile:** script que contém uma série de instruções para construir uma imagem Docker. Define o ambiente necessário para o aplicativo e como ele deve ser configurado.
- **Docker Hub:** registro público de imagens Docker, sendo possível encontrar e compartilhar imagens de containers ou apenas armazená-las.
- **Docker COmpose:** ferramenta para definir e gerenciar aplicações multi-container (utilizam múltiplos contêineres Docker para compor uma única aplicação).

#### Usos
- **Desenvolvimento e Teste:** Docker permite que os desenvolvedores criem ambientes de desenvolvimento e teste que são idênticos aos ambientes de produção, reduzindo problemas de compatibilidade e facilitando o desenvolvimento ágil.
- **Implantação de Aplicações:** Containers Docker proporcionam uma forma consistente e portátil para implantar aplicativos em diferentes ambientes, desde máquinas locais até servidores em nuvem.

### Kubernetes
Plataforma de código aberto para manejamento de containers que automatizam a implantação, o dimensionamento e a operação de aplicativos em containers.

#### Características Principais
- **Orquestração de Containers:** Automatiza o gerenciamento de containers em um cluster, incluindo implantação, atualização, balanceamento de carga e recuperação de falhas
- **Escalabilidade:** Permite o dimensionamento automático de aplicações, tanto horizontalmente (adicionando mais instâncias de containers) quanto verticalmente (ajustando os recursos alocados para os containers).
- **Desdobramento e Atualização:** Facilita a implantação contínua e a atualização de aplicativos, suportando estratégias como rolling updates e rollbacks.
- **Auto-recuperação:** Monitora a saúde dos containers e dos nós do cluster, substituindo automaticamente containers com falhas e redistribuindo cargas conforme necessário.
- **Gerenciamento de Configuração e Segredos:** Oferece mecanismos para gerenciar configurações e segredos de forma segura, separando dados de configuração do código e garantindo a segurança das credenciais.
- **Serviços e Networking:** Fornece abstração para serviços, balanceamento de carga e descoberta de serviços dentro do cluster, facilitando a comunicação entre containers e aplicações.

#### Componentes
- **Pod:** A menor unidade de implantação no Kubernetes. Um Pod pode conter um ou mais containers que compartilham armazenamento e rede.
- **Node:** Uma máquina física ou virtual que executa os Pods e fornece os recursos necessários para a execução dos containers. Cada Node é gerenciado pelo Master e executa o Kubelet e o container runtime.
- **Cluster:** Um conjunto de Nodes que trabalham juntos para executar aplicações em containers. O cluster é gerenciado por um Master Node e pode incluir múltiplos Nodes.
- **Master Node:** O nó central que coordena o cluster, gerenciando a comunicação entre os Nodes e controlando a execução dos Pods. Ele contém componentes como o API Server, o Controller Manager e o Scheduler.
- **Deployment:** Um objeto que define a forma como um conjunto de Pods deve ser executado e gerenciado, incluindo as estratégias de atualização e a escala.
- **Service:** Um abstração que define um conjunto de Pods e fornece uma forma de expor esses Pods como um serviço de rede, permitindo a comunicação entre eles e com o mundo externo.
- **ConfigMap e Secret:** Objetos que permitem a separação de configurações e informações sensíveis do código dos containers, facilitando o gerenciamento e a segurança das configurações.
- **Ingress:** Gerencia o acesso externo aos serviços dentro do cluster, fornecendo balanceamento de carga e roteamento de tráfego baseado em regras de URL e host.

### CI/CD
CI/CD: Integração Contínua, Entrega Contínua, Deploy Contínuo.
São práticas fundamentais que visam melhorar a qualidade e a eficiência do ciclod e vida do desenvolvimento de software. Com essas práticas, é possível automatizar o processo e permitir que as equipes entreguem atualizações mais rápidas e confiáveis.

#### Conceito
**1. Integração Contínua:** Integrar alterações de código em um repositório compartilhado com o intuito de garantir que as alterações sejam compatíveis com o código existente, melhorando a qualidade e estabilidade do software.
   - **Processo:**
      - Desenvolvedores fazem alterações no código e as enviam para o repositório.
      - O sistema de CI detecta as mudanças, executa um build do código e realiza testes automatizados.
      - Relatórios são gerados para indicar o sucesso ou falha dos testes, permitindo correção rápida de problemas.
   
**2. Entrega Contínua:** Manter o código em um estado onde ele possa ser implementado a qualquer momento, sempre pronto para ser lançado, minimizando o risco e o tempo de entrega.
   - **Processo:**
      - Após a integração contínua, o código é automaticamente implantado em um ambiente de staging.
      - Testes adicionais são executados em staging para garantir que o software esteja pronto para produção.
      - Se os testes forem bem-sucedidos, o código está pronto para ser implantado em produção.
     
**3. Deploy Contínuo:** Extensão da entrega contínua, com o objetivo de automatizar por completo o processo de implantação, permitindo que atualizações de software sejam entregues aos usuários finais mais rapidamente e com mais frequência.
   - **Processo:**
      - O código é integrado e testado automaticamente.
      - Se todos os testes forem aprovados, a implantação para produção é realizada sem intervenção manual.

Assim é possível reduzir o risco de problemas em produções, acelerar o processo de entrega e garantir consistencia no processo de construção e implantação.


**Desafios Encontrados:**  
Compreender alguns temas específicos.

**Feedback e Ajustes:**  
Mais indicações de materiais para o estudo dos assuntos, principalmente artigos.

**Próximos Passos:**  
Vou focar em realizar testes
